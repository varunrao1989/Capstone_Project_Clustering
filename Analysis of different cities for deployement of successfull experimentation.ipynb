{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A description of the problem and a discussion of the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction/Business Problem\n",
    "When companies deploy new projects, it is done through experimentation. An experiment is run in a controlled environment OR in many cases in a suitable pilot location, and if successful, it is expanded to other locations. While deciding what other locations one should deploy such projects, it is always beneficial to have a good comparison of target locations with the location already experimented on, based on appropriate parameters. This can be achieved by realizing how similar or dis-similar the locations are. \n",
    "\n",
    "In this project we will aim to solve three seperate scenarios (having the same problem above) through Clustering:\n",
    "\n",
    "1.A company like Amazon/Walmart were thinking long-term and wanted to have their own logistics fleet for better control and reliability. They ran an experiment in New York City to determine where to establish new package drop-off locations, to make it more convinient for their customers to return something. The experiment was a success and they have regonized that best target cities are Toronto and San Francisco.\n",
    "We solve the problem of what areas in Toronto and San Francisco should they target to set up these drop off locations.\n",
    "\n",
    "2.A company like Trip Advisor wants to recommend me locations in San Francisco based on what I did in New York City.\n",
    "We solve the problem of recommending locations in a new city based on similarities in the previous one.\n",
    "\n",
    "3.Clusters in different cities can also help companies determine where, in different cities, they can deploy self driving cars to pilot test (based on successful experimentation for pilot tests in one city). Lets consider Waymo, Uber or Tesla are at that stage where level 5 cars can be tested on roads with real customers. They would like to know where they can deploy their cars, after a successful test (again) in New York City. They have identified that San Francisco and Toronto are two metros that are open to this.\n",
    "OR This can even help RideShare companies like Uber and Lyft to deploy more cars around a particluar area specifically for a certain type of service (like Ride Share). So people will much rather take a shared Uber/Lyft rather than taking a bus for example. \n",
    "Here we solve the problem of Identifying where they can deploy specific type of service in San Francisco based on experimentation in NYC.\n",
    "\n",
    "*Note there are a lot of assumptions made to create these scenarios and the focus of \"this\" work is clustering Neighborhoods based on top 10 venues (in each neighborhood) in these three cities to see what are similar areas between the three and how they can be identified after the target locations (San Francisco and Toronto) have been identified.\n",
    "\n",
    "** This study does not go into too much detail, and does not take into consideration a lot of real world factors and other data. This is meant to be a presentation of Data Science and Machine Learning abilities to showcase academic comprehension of the subject matter and tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A description of the data and how it will be used to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In order to cluster different areas of the three cities mentioned above, we will consider each area within a city as a \"neighborhood\". The data set for all three cities was cleaned from sources (below), to represent the 'Neighborhood' with corresponding latitudes and longitudes of each neighborhood's approximate center and is stored in \"combined_df\" dataframe. Details on three Indivudual data sources:\n",
    "\n",
    "1. Toronto DataSet: This dataframe in csv was combined using two different datasets:\n",
    "    - Postal Codes List in Toronto (which was exported and saves as an xslx file). This file contained the postal codes and corresponding Neighborhood names. https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M\n",
    "    - Geospatial File that has the latitude and longitudes and has been provided by Coursera http://cocl.us/Geospatial_data\n",
    "2. New York DataSet: This data set was extracted from https://geo.nyu.edu/catalog/nyu_2451_34572\n",
    "3. San Francisco DataSet: This data was extracted from https://geodata.lib.berkeley.edu/catalog/ark28722-s75c8t\n",
    "\n",
    "We will use the three datasets to create one Merged dataset that has all neighborhoods of all three cities. We will extract Venues for each neighborhood from Foursquare and cluster the neighborhoods to determine similarities in inter-city neighborhoods and intra-city neighborhoods. We will focus more in \"inter-city\" relationships as it will be used to solve all three problems stated in the first section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHODOLOGY\n",
    "\n",
    "This section explains the main body of the work conducted in the project. It is divided into 6 sub-sections (if you want to follow along in the notebook). After these sub-sections, the next main section \"RESULTS\", will be portrayed.\n",
    "\n",
    "Before diving into details, to summarize the Methodology, Neighborhood and Location data for three cities (NYC, SF and Toronto) was taken and combined into a single dataframe. Venues were extracted for these Neighborhods from FourSquare and each Neighborhood was explored based on top venues. Then, partitioning clustering (K-means) was used to cluster the neighborhoods into 'four' clusters to see similarity of clusters between different neighborhoods in different cities. The result of this clustering will enable one to make judgements about what location - in new cities (SF, Toronto) can be used to deploy an experiment that succeeeded in a pilot-city (NYC).\n",
    "\n",
    "1. Importing and preparing all three data sets (NY, SF, Toronto): As mentioned in the data section, all three datasets were publically available and were formatted and cleaned to get a conbined dataframe \"combined_df\" which consisted of four columns, 'City', 'Neighborhood', 'Latitude', and 'Longitude'. \n",
    "\n",
    "2. Creating Maps and Pinouts to visualize the data: A Map of North America was created to visualize the Neighborhood in three cities using Folium. This ensured all the neighborhoods were imported properly and the location data was correct.\n",
    "\n",
    "3. Using FourSquare to get the Venue information for combined_df: FourSquare is an application that has data for Venues and Users and Experiences. Using this app, data for Venues (limited to 100 venues per neighborhod) was extracted using API calls (more on APIs here https://developer.foursquare.com/docs/). The main interest of this project was to segment the Neighborhods based on different types of Venues in that Neighborhood. Hence 'Venue_Categories' were exported and cleaned up to create a master_df contianing all Cities, Neighborhoods, Venues and Venue Categories, along with their location information.\n",
    "\n",
    "4. Create dataframe for Clustering: The goal of this sub-section, was to have a dataframe, that could be used for Clustering. This data frame is \"master_grouped\" and was created by grouping \"one_hot encoded venues table\", by Neighborhood and City to maintain uniqueness of each Neighborhood. Also another dataframe was created created, that would show the top 10 Venues by Neighborhood, so we can add our Cluster labels to this dataframe to analyze what each cluster contains (after clustering). This dataframe is \"neighborhoods_venues_sorted\".\n",
    "\n",
    "5. Clustering: Once the dataframes above were available, partitioning Clustering (K-means) was used to create 6 clusters of Neighborhoods. Cluster labels were inserted to the \"neighborhoods_venues_sorted\" and this dataframe was merged with \"combined_df\" which contained latitudes and longitudes (location) information and a final dataframe \"master_merged\" was created that could be analyzed for results.\n",
    "\n",
    "6. Created a MAP for visualization: Finally, another Map was created using folium to visualize the clusters and how they were spread in different cities. Toronto and NYC being close by, we could see the spread to compare both of them pretty conviniently. But SF being on the opposite coast, we comapred them using snippets. \n",
    "\n",
    "             * The next sections are cells for METHODOLOGY section. After sub section 6, RESULTS are shared. You will need to install folium and run the cells to view Maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # library to handle JSON files\n",
    "\n",
    "!conda install -c conda-forge geopy --yes\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "!conda install -c conda-forge folium=0.5.0 --yes\n",
    "import folium # map rendering library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing and preparing all three data sets (NY, SF, Toronto).\n",
    "In order to create maps using geoplot and identifying neighborhoods on it, we need to get dfs that have neighborhood, latitude and longitude.\n",
    "\n",
    "toronto_df  \n",
    "newyork_df  \n",
    "sf_df  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Creating Toronto Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toronto_df = pd.read_csv(r\"C:\\Users\\The Godfather\\Desktop\\totonto_neighborhoods.csv\")\n",
    "toronto_df.drop(columns=[\"Unnamed: 0\",\"PostalCode\"], inplace=True)\n",
    "toronto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding City column\n",
    "toronto_df[\"City\"] = \"Toronto\"\n",
    "toronto_df = toronto_df[[\"City\", \"Neighborhood\", \"Latitude\", \"Longitude\"]]\n",
    "toronto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Creating NYC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1.2.1 Importing the json file\n",
    "with open(r\"C:\\Users\\The Godfather\\Desktop\\NYC_neighborhood_names-geojson.json\") as json_data:\n",
    "    newyork_data = json.load(json_data)\n",
    "    \n",
    "newyork_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_data = newyork_data['features']\n",
    "newyork_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2.2. Converting the above details into a dataframe\n",
    "# define the dataframe columns\n",
    "column_names = ['Borough', 'Neighborhood', 'Latitude', 'Longitude'] \n",
    "\n",
    "# instantiate the dataframe\n",
    "newyork_df = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in newyork_data:\n",
    "    borough = neighborhood_name = data['properties']['borough'] \n",
    "    neighborhood_name = data['properties']['name']\n",
    "        \n",
    "    neighborhood_latlon = data['geometry']['coordinates']\n",
    "    neighborhood_lat = neighborhood_latlon[1]\n",
    "    neighborhood_lon = neighborhood_latlon[0]\n",
    "    \n",
    "    newyork_df = newyork_df.append({'Borough': borough,\n",
    "                                          'Neighborhood': neighborhood_name,\n",
    "                                          'Latitude': neighborhood_lat,\n",
    "                                          'Longitude': neighborhood_lon}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding City column\n",
    "newyork_df[\"City\"] = \"NYC\"\n",
    "newyork_df = newyork_df[[\"City\", \"Neighborhood\", \"Latitude\", \"Longitude\"]]\n",
    "newyork_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Creating San Francisco Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(r\"C:\\Users\\The Godfather\\Desktop\\sf_neighborhood_bounds-geojson.json\") as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data['features'][0]['geometry']['coordinates'][0][0]\n",
    "#data['features'][0]['properties']['NEIGHBORHO']\n",
    "DDFF=pd.DataFrame()\n",
    "for i in range(0, len(data['features'])):\n",
    "    df= pd.DataFrame(data['features'][i]['geometry']['coordinates'][0][0]).rename({0:\"Longitude\", 1:\"Latitude\"}, axis=1)\n",
    "    df['Neighbrhood'] = data['features'][i]['properties']['NEIGHBORHO']\n",
    "    DDFF = DDFF.append(df).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = DDFF.groupby(['Neighbrhood']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = sf_df[['Neighbrhood', 'Latitude', 'Longitude']]\n",
    "sf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df[\"City\"] = \"SF\"\n",
    "sf_df = sf_df[[\"City\", \"Neighbrhood\", \"Latitude\", \"Longitude\"]]\n",
    "sf_df = sf_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df.rename(columns ={\"Neighbrhood\":\"Neighborhood\"}, inplace=True)\n",
    "sf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Need to make NY and Toronto df same as SF by dropping Boroughs. AND Merging the three data frames to create new combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now merge all three dfs vertically\n",
    "combined_df = pd.concat([sf_df, newyork_df, toronto_df])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are a total of 445 Neighborhoods in all three cities combined that we will take into consideration. Of which 431 are unique.\n",
    "We will however consider the duplicate cases at this stage as the 'City' column will make the row unique for further processing. We will see later that this gets weeded out step by step and we ignore the few neighborhoods while clustering and get a clean set of 423 rows (after removing the NaNs from one_hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Maps and Pinouts to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'USA'\n",
    "geolocator = Nominatim(user_agent=\"usa-explorer\")\n",
    "\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of USA are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map of USA/Canada using latitude and longitude values\n",
    "map_northamerica = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, neighborhood in zip(combined_df['Latitude'], combined_df['Longitude'], combined_df['Neighborhood']):\n",
    "    label = '{}'.format(neighborhood)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_northamerica)  \n",
    "    \n",
    "map_northamerica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using FourSquare to get the Venue information for combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Create foursquare credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = '4YRRGTHC2QBF0UUR1ASAYR00SZWDMNXI2GN2SI0GTVSEZMBM' # your Foursquare ID\n",
    "CLIENT_SECRET = 'RGUUUSUTMV5SIBTY1AAN2Y1G5NESQNQX0VSWR1S41DJT1SMM' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Create a function to get all nearby venues in combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100\n",
    "\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_venues = getNearbyVenues(names=combined_df['Neighborhood'],\n",
    "                                   latitudes=combined_df['Latitude'],\n",
    "                                   longitudes=combined_df['Longitude']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_venues.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are a total of 12946 Venues that were exported from Foursquare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Add the city column from combined_df into all_venues and create a new dataframe called \"master_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.merge(all_venues, combined_df.drop_duplicates().copy().rename({'Latitude':'Neighborhood Latitude','Longitude':'Neighborhood Longitude'},axis=1), on=['Neighborhood','Neighborhood Longitude','Neighborhood Latitude'], how='left').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Make sure data is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see number of unique categories that were returned from four square\n",
    "\n",
    "print('There are {} uniques categories.'.format(len(master_df['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master_df.loc[master_df['Neighborhood'] == 'NaN'])\n",
    "print(master_df.loc[master_df['Neighborhood'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create dataframe for Clustering\n",
    "1. Create a grouped dataframe by 'City' and 'Neighborhood'with one_hot encoded venue categories.\n",
    "2. Cluster them using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df.groupby(['Neighborhood', 'City']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "master_onehot = pd.get_dummies(master_df[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "\n",
    "#add neighborhood and city column back to dataframe\n",
    "master_onehot[['Neighborhood', 'City']] = master_df[['Neighborhood', 'City']] \n",
    "\n",
    "# move city column to the first column\n",
    "fixed_columns = [master_onehot.columns[-1]] + list(master_onehot.columns[:-1])\n",
    "master_onehot = master_onehot[fixed_columns]\n",
    "\n",
    "# move neighborhood column to the first column\n",
    "#fixed_columns = [master_onehot.columns[306]] + list(master_onehot.columns[:-1])\n",
    "#master_onehot = master_onehot[fixed_columns]\n",
    "\n",
    "master_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master_onehot.columns.get_loc(\"Neighborhood\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_onehot[\"Neighborhood\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will notice each neighborhood has multiple rows. This is because one hot was done by venue category. That means, if a neighborhood has 5 venue categories, it will have 5 rows, with '1's on corresponding columns on each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_grouped = master_onehot.groupby([\"Neighborhood\", \"City\"]).mean().reset_index()\n",
    "master_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now lets see what are the top 5 venues in each neighborhood. Remove the \"City\" column\n",
    "\n",
    "num_top_venues = 5\n",
    "\n",
    "for hood in master_grouped['Neighborhood']:\n",
    "    print(\"----\"+hood+\"----\")\n",
    "    temp = master_grouped[master_grouped['Neighborhood'] == hood].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[2:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 2})\n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets put the above info in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[2:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['Neighborhood'] = master_grouped['Neighborhood']\n",
    "\n",
    "for ind in np.arange(master_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(master_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_venues_sorted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 6\n",
    "\n",
    "\n",
    "master_grouped_clustering = master_grouped.drop(columns =['Neighborhood', 'City'])\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(master_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "master_merged = combined_df\n",
    "\n",
    "# merge master_grouped with combined_df to add latitude/longitude for each neighborhood\n",
    "master_merged = master_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
    "\n",
    "master_merged.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_merged = master_merged.drop_duplicates(subset=\"Neighborhood\")\n",
    "master_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_merged['Cluster Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_merged.loc[master_merged['Cluster Labels'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_merged.drop(index=[207,257,5,11,95], inplace=True)\n",
    "master_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_merged[\"Cluster Labels\"] = master_merged[\"Cluster Labels\"].apply(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_merged['Cluster Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a MAP to visualize if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(master_merged['Latitude'], master_merged['Longitude'], master_merged['Neighborhood'], master_merged['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS & DISCUSSIONS\n",
    "\n",
    "The data successfully divided the Neighborhoods in all three cities into clusters. Clusters 0 to 5. Of which three clusters will be considered for analysis (0, 2 & 5)\n",
    "\n",
    "1.Cluster 0 (RED Colored Clusters): Thses were areas where the most popular venue was a Bus Stops. There were two main locations in SF that fit this category. This is a bit surprising but these come up as a lot of Students/Tourists who want to travel from the City to these two destinations, end up getting here taking multiple modes of transport and prefer Buses (single mode of transport, even though its slower) to get back to their residence as they are tired after a day in the beach or walking/hiking around twin peaks.\n",
    "\n",
    "2.Clsuter 1 (PURPLE Colored Clusters): Just had one location. So we will not consider this.  \n",
    "\n",
    "3.Clsuter 2 (DARK BLUE Colored Clusters): These areas had trails, parks, playgrounds and Zoos. These are places where people visit to for  outdor activities or as weekend trips. Also a lot of tourists visit these places to see local nature and points of interest. This cluster is a good recommendation that companies like Trip Advisor will give to a person who enjoys outdoor activities and is visiting San Francisco or Toronto.\n",
    "\n",
    "4.Clsuter 3 (LIGHT BLUE Colored Clusters): Just had 2 locations. So we will not consider this.  \n",
    "\n",
    "5.Clsuter 4 (GREEN Colored Clusters): Pizza Places. Not of much use for our applications. So we will not consider this.  \n",
    "\n",
    "6.Clsuter 5 (ORANGE Colored Clusters): Restaurants, Coffee Shops, Cafe's, Convenience Stores, Pubs & Bars, etc. This is the most prominent cluster as all three locations are densly populated cities. These are the places where lot of movement happens. People are constantly going to Cafe's and Coffee shops on a regular basis. Neighborhood specific Cafe's, Cofee Shops, Bakeries and Restaurants serve as a great place for people to regularly go with friends and family. These locations can be used by Amazon/Walmart to establish package drop-off locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cluster [0]\n",
    "master_merged.loc[master_merged['Cluster Labels'] == 0, master_merged.columns[[0,1] + list(range(5, master_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster [1]:\n",
    "master_merged.loc[master_merged['Cluster Labels'] == 1, master_merged.columns[[0,1] + list(range(5, master_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cluster [2]\n",
    "master_merged.loc[master_merged['Cluster Labels'] == 2, master_merged.columns[[0,1] + list(range(5, master_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster [3]\n",
    "master_merged.loc[master_merged['Cluster Labels'] == 3, master_merged.columns[[0,1] + list(range(5, master_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster [4]\n",
    "master_merged.loc[master_merged['Cluster Labels'] == 4, master_merged.columns[[0,1] + list(range(5, master_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster [5]\n",
    "master_merged.loc[master_merged['Cluster Labels'] == 5, master_merged.columns[[0,1] + list(range(5, master_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "This study sucessfully segmented Neighborhoods, based on similarities in most popular Venues (between cities) which helped companies make decisions on waht area to deploy a project in a new city based on successful experimentation in a pilot - city.\n",
    "This was extended to three cases introduced in the first section.\n",
    "\n",
    "1.Companies like Amazon/Walmart were able to deploy \"package drop-off locations\" around Neighborhood venues that belong to cluster 5 (Orange Colored Clusters) in Toronto and San Francisco.\n",
    "\n",
    "2.Trip Advisor (or similar) was able to give good relevant recommends - cluster 2 (Dark Blue Colored Clusters) to to its users who were into outdoor activities and were either tourists or just exploring the city of Toronto and San Francisco.\n",
    "\n",
    "3.Companies like Uber/Lyft or SuperShuttle were able to deploy new Ride Share service routes in San Francisco - cluster 0 (Red Colored Clusters) where majority of the population would wait around a bus stop and get a single mode of transportation, rather than use other modes of transport where they would have to change staions/modes multiple times.\n",
    "\n",
    "\n",
    "** Again, this study does not go into too much detail, and does not take into consideration a lot of realworld factors and other data. This is meant to be a presenttion of Data Science and Machine Learning abilities to showcase academic comprehension of the subject and tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
